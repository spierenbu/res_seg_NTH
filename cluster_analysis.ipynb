{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lulu/.pyenv/versions/3.9.7/envs/study_1_venv/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_exposure = '../data/results/exposure/'\n",
    "dir_adjacency_matrix = '../data/processed_data/adjacency_matrix/'\n",
    "dir_adjacency_matrix_edited = '../data/processed_data/adjacency_matrix/edited/'\n",
    "dir_zones = '../data/processed_data/zones_delineation/'\n",
    "dir_zones_edited = '../data/processed_data/zones_delineation/edited/'\n",
    "dir_cov_mat = '../data/processed_data/covariance_matrix/'\n",
    "dir_gemeente = '../data/processed_data/city_boundary/'\n",
    "\n",
    "parameters = pd.read_csv('parameter.csv')\n",
    "parameters = parameters.set_index('variable')\n",
    "dissimilarity_threshold = parameters.loc['dissimilarity_threshold','value']\n",
    "\n",
    "list_file = os.listdir(dir_gemeente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_analysis(zones,adjacency_matrix,exposure,dissimilarity_threshold):\n",
    "   \n",
    "    '''\n",
    "    This function aggregates spatial units into homogeneous regions.\n",
    "\n",
    "           Parameters:\n",
    "                    zones (gpd.GeoDataFrame): Spatial units.\n",
    "                    adjacency_matrix (np.array): Matrix indicating the adjacency of spatial units.\n",
    "                    exposure (pd.DataFrame): Exposure in each spatial unit.\n",
    "                    dissimilarity_threshold (float): stopping criterion.\n",
    "\n",
    "            Returns:\n",
    "                    zones (gpd.GeoDataFrame): Spatial units with the region they belong to.\n",
    "\n",
    "    '''\n",
    "   # N_component, labels = sparse.csgraph.connected_components(adjacency_matrix)\n",
    "   # if N_component > 1:\n",
    "   #     print(city + ' has a problem')\n",
    "    zones = zones.copy()\n",
    "    zones = zones.loc[:,['id_unit','geometry']]\n",
    "    zones = zones.merge(exposure, on = 'id_unit', how = 'left')\n",
    "    zones['expos_NW'] = zones['expos_NW'].mask(zones['expos_NW'].isna(),zones['share_NW_c'].mean())\n",
    "    zones['expos_rel'] = zones['expos_NW']/zones['share_NW_c']\n",
    "    zones['expos_rel'] = zones['expos_rel'].mask(zones['share_NW_c']==0,0)\n",
    "    X = np.array([zones.loc[:,'expos_NW']]).T\n",
    "\n",
    "    clustering = AgglomerativeClustering(distance_threshold=dissimilarity_threshold, \n",
    "                                        n_clusters=None,\n",
    "                                        connectivity=adjacency_matrix,\n",
    "                                        linkage = 'ward').fit(X)\n",
    "\n",
    "    zones['region'] = clustering.labels_\n",
    "\n",
    "    # Some regions are not fully contiguous. They are composed by adjacent regions that touch themselves only on one point.\n",
    "    # We explode these regions into sub regions.\n",
    "    regions = zones.dissolve(by ='region').reset_index()\n",
    "    regions = regions.explode(index_parts=False).reset_index(drop = True)\n",
    "    regions['region'] = regions.index.copy()\n",
    "\n",
    "    # Adding the id of the new regions to the zones dataframe.\n",
    "    zones = zones.drop(columns = 'region')\n",
    "    zones_reg = zones.overlay(regions[['geometry','region']],\n",
    "                              how = 'intersection', \n",
    "                              keep_geom_type = True)\n",
    "    # With the overlay function, zones might get duplicated. Keeping only the largest duplicate.\n",
    "    zones_reg['rank'] = zones_reg.area\n",
    "    zones_reg = zones_reg.sort_values(by = 'rank', ascending = False)\n",
    "    zones_reg = zones_reg.drop_duplicates(subset = 'id_unit')\n",
    "    # Merging with the zones file to keep the initial geometry.\n",
    "    zones = zones.merge(zones_reg[['id_unit','region']], on = 'id_unit', how = 'left')\n",
    "    if not zones.loc[zones['region'].isna()].empty:\n",
    "        raise Exception('Error, some zones are not associated to any cluster.')\n",
    "\n",
    "\n",
    "    zones['weighted_expos'] = zones['expos_NW'] * zones['pop_res']\n",
    "    zones['expos_std'] = np.sqrt(zones['expos_NW'].var())\n",
    "\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_conc_region(zones, cov_mat, reg_name):\n",
    "\n",
    "    '''\n",
    "    This function computes the standard deviation of exposure in a region.\n",
    "\n",
    "           Parameters:\n",
    "                    zones (gpd.GeoDataFrame): Spatial units.\n",
    "                    cov_mat (np.array): Covariance matrix of the exposure.\n",
    "                    reg_name (str): name of the column containing the region information in the zones input.\n",
    "\n",
    "            Returns:\n",
    "                    reg_av_std (gpd.GeoDataFrame): Regions with the standard deviation of the exposure.\n",
    "               \n",
    "    '''\n",
    "    tot_pop_region = zones[['pop_res',reg_name]].groupby(by = reg_name).sum().reset_index()\n",
    "\n",
    "    zones = zones.merge(tot_pop_region.rename(columns = {'pop_res':'pop_' + reg_name}), on = reg_name)\n",
    "    # weight_pop corresponds to the theta in the technical report.\n",
    "    zones['weight_pop'] = zones['pop_res'] / zones['pop_' + reg_name]\n",
    "\n",
    "    weight_id_unit = zones.set_index('id_unit')['weight_pop'].copy()\n",
    "\n",
    "    cov_mat = cov_mat.multiply(weight_id_unit, axis = 0)\n",
    "    cov_mat = cov_mat.multiply(weight_id_unit, axis = 1)\n",
    "\n",
    "    reg_av_std = zones[[reg_name,'access_NW','access_tot',\n",
    "                        'N_NW_res','pop_res','geometry']].dissolve(by = reg_name,\n",
    "                                                                   aggfunc = 'sum').reset_index()\n",
    "    reg_av_std['std_ind'] = 0\n",
    "\n",
    "    # Computing the corrected standard deviation of the mean indicator.\n",
    "    for i in zones[reg_name].unique():\n",
    "\n",
    "        var_mean_ind = cov_mat.loc[cov_mat.index.isin(zones.loc[zones[reg_name] == i,'id_unit']),\n",
    "                                   cov_mat.columns.isin(zones.loc[zones[reg_name] == i,'id_unit'])].to_numpy().sum()\n",
    "        reg_av_std['std_ind'] = reg_av_std['std_ind'].mask(reg_av_std[reg_name] == i, np.sqrt(var_mean_ind))\n",
    "\n",
    "    reg_av_std['mean_ind'] = zones.loc[0,'share_NW_c']\n",
    "    reg_av_std.loc[:,'mean_ind'] = reg_av_std['mean_ind'].mask(reg_av_std['access_tot'] > 0, reg_av_std['access_NW'] / reg_av_std['access_tot'])\n",
    "\n",
    "    reg_av_std['ratio_sig'] = 0\n",
    "    reg_av_std.loc[:,'ratio_sig'] = reg_av_std['ratio_sig'].mask(reg_av_std['std_ind'] > 0,\n",
    "                                                                (reg_av_std['mean_ind'] - zones.loc[0,'share_NW_c']) / reg_av_std['std_ind']) \n",
    "\n",
    "    return reg_av_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_size_seg(zones,city,cov_mat):\n",
    "\n",
    "    '''\n",
    "    This function\n",
    "        - labels regions.\n",
    "        - merges adjacent regions that are labeled the same into larger ones.\n",
    "        - computes some statistics for each region.\n",
    "\n",
    "           Parameters:\n",
    "                    zones (gpd.GeoDataFrame): Spatial units.\n",
    "                    city (str): Name of the city considered.\n",
    "                    cov_mat (np.array): Covariance matrix of the exposure.\n",
    "\n",
    "            Returns:\n",
    "                    new_regions (gpd.GeoDataFrame): Regions labeled.\n",
    "               \n",
    "    '''\n",
    "    \n",
    "    if zones.loc[0,'share_NW_c'] == 0:\n",
    "        size_seg = pd.DataFrame({'city':[city],\n",
    "                                'area_seg':[0],\n",
    "                                'area_seg_rel':[0],\n",
    "                                'pop_res':[0],\n",
    "                                'pop_rel':[0],\n",
    "                                'NW_rel':[0],\n",
    "                                'weighted_expos':[0],\n",
    "                                'expos_rel':[1]})\n",
    "        return size_seg\n",
    "   \n",
    "    size_seg = std_conc_region(zones, cov_mat, 'region')\n",
    "   \n",
    "    size_seg['seg'] = 0\n",
    "    size_seg.loc[:,'seg'] = size_seg['seg'].mask(size_seg['ratio_sig'] > 1, 1)\n",
    "    size_seg.loc[:,'seg'] = size_seg['seg'].mask(size_seg['ratio_sig'] < -1, -1)\n",
    "\n",
    "    # Spatial delineation of the regions (we aggregate spatially regions that are labelled the same way).\n",
    "    new_regions = size_seg[['seg','geometry']].dissolve(by = 'seg', aggfunc = 'sum').reset_index()\n",
    "    new_regions = new_regions.explode(ignore_index = True)\n",
    "    new_regions['new_region'] = new_regions.index.copy()\n",
    "\n",
    "    zones_reg = zones.overlay(new_regions[['geometry','new_region']],\n",
    "                              how = 'intersection', \n",
    "                              keep_geom_type = True)\n",
    "\n",
    "    zones_reg['rank'] = zones_reg.area\n",
    "    zones_reg = zones_reg.sort_values(by = 'rank', ascending = False)\n",
    "    zones_reg = zones_reg.drop_duplicates(subset = 'id_unit')\n",
    "    zones = zones.merge(zones_reg[['id_unit','new_region']], on = 'id_unit', how = 'left')\n",
    "\n",
    "    if not zones.loc[zones['new_region'].isna()].empty:\n",
    "        raise Exception('Error, not all zones in a region')\n",
    "\n",
    "    new_regions = std_conc_region(zones, cov_mat, 'new_region')\n",
    "\n",
    "    new_regions['seg'] = 0\n",
    "    new_regions.loc[:,'seg'] = new_regions['seg'].mask(new_regions['ratio_sig'] > 1, 1)\n",
    "    new_regions.loc[:,'seg'] = new_regions['seg'].mask(new_regions['ratio_sig'] < -1, -1)\n",
    "\n",
    "    new_regions['area_seg'] = new_regions.area / 1e6\n",
    "    new_regions['area_seg_rel'] = new_regions.area / zones.area.sum()\n",
    "    new_regions['pop_rel'] = new_regions['pop_res'] / zones['pop_res'].sum()\n",
    "    new_regions['NW_rel'] = new_regions['N_NW_res'] / zones['N_NW_res'].sum()\n",
    "    new_regions['pop_city'] = zones['pop_res'].sum()\n",
    "    new_regions['NW_city'] = zones['N_NW_res'].sum()\n",
    "    new_regions['weighted_expos'] =  0\n",
    "    new_regions['weighted_expos'] = new_regions['weighted_expos'].mask(new_regions['access_tot'] >0,\n",
    "                                                                       new_regions['access_NW'] / new_regions['access_tot'])\n",
    "    new_regions['expos_rel'] = new_regions['weighted_expos'] / zones.loc[0,'share_NW_c']\n",
    "    new_regions['city'] = city\n",
    "    new_regions['share_NW_c'] = zones.loc[0,'share_NW_c']\n",
    "    \n",
    "    return new_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='388' class='' max='388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [388/388 13:24<00:00 Gennep]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "area_seg_tot = pd.DataFrame({'city':[],'area_seg':[],'area_seg_rel':[],'pop_res':[],\n",
    "                             'pop_rel':[],'NW_rel':[],'weighted_expos':[],'expos_rel':[]})\n",
    "\n",
    "list_file.remove('README_city_boundary.mkd')\n",
    "list_file.remove('Buitenland.gpkg')\n",
    "pb = progress_bar(range(len(list_file)))\n",
    "\n",
    "for i in pb:\n",
    "    file = list_file[i]\n",
    "    city = file[:-5]\n",
    "    pb.comment = city\n",
    "\n",
    "    if city == 'Baarle_Nassau':\n",
    "        continue\n",
    "    # This city have two pieces.\n",
    "    if city == 'Amsterdam':\n",
    "        continue\n",
    "        exposure_1 = pd.read_csv(dir_exposure + city + '_exposure_1.csv')\n",
    "        adjacency_matrix_1 = sparse.load_npz(dir_adjacency_matrix_edited + city \n",
    "                                             + '_adjacency_matrix_1.npz')\n",
    "        zones_1 = gpd.read_file(dir_zones_edited + 'PC_'+ city + '_1.gpkg')\n",
    "        zones_1 = cluster_analysis(zones_1,adjacency_matrix_1,exposure_1, \n",
    "                                   dissimilarity_threshold*np.sqrt(len(zones_1)))\n",
    "        \n",
    "        exposure_2 = pd.read_csv(dir_exposure + city + '_exposure_2.csv')\n",
    "        adjacency_matrix_2 = sparse.load_npz(dir_adjacency_matrix_edited + city \n",
    "                                             + '_adjacency_matrix_2.npz')\n",
    "        zones_2 = gpd.read_file(dir_zones_edited + 'PC_'+ city + '_2.gpkg')\n",
    "        zones_2 = cluster_analysis(zones_2,adjacency_matrix_2,exposure_2, \n",
    "                                   dissimilarity_threshold*np.sqrt(len(zones_2)))\n",
    "        zones_2['region'] = zones_2['region'] + zones_1['region'].max() + 1\n",
    "        \n",
    "        zones = pd.concat([zones_1,zones_2], ignore_index = True)\n",
    "        \n",
    "    elif os.path.isfile(dir_zones_edited + 'PC_' + file):\n",
    "        exposure = pd.read_csv(dir_exposure + city + '_exposure.csv')\n",
    "        adjacency_matrix = sparse.load_npz(dir_adjacency_matrix_edited + city \n",
    "                                           + '_adjacency_matrix.npz')\n",
    "        zones = gpd.read_file(dir_zones_edited + 'PC_'+ file)\n",
    "        zones = cluster_analysis(zones,adjacency_matrix, exposure, \n",
    "                                 dissimilarity_threshold*np.sqrt(len(zones)))\n",
    "        \n",
    "    elif os.path.isfile(dir_zones + 'PC_' + file):\n",
    "        exposure = pd.read_csv(dir_exposure + city + '_exposure.csv')\n",
    "        adjacency_matrix = sparse.load_npz(dir_adjacency_matrix + city \n",
    "                                           + '_adjacency_matrix.npz')\n",
    "        zones = gpd.read_file(dir_zones + 'PC_'+ file,layer = 'zone')\n",
    "        zones = cluster_analysis(zones,adjacency_matrix, exposure, \n",
    "                                 dissimilarity_threshold*np.sqrt(len(zones)))\n",
    "   \n",
    "    cov_mat = sparse.load_npz(dir_cov_mat + city + '_cov_mat.npz')\n",
    "    cov_mat = cov_mat.todense()\n",
    "    cov_mat_ind = pd.read_csv(dir_cov_mat + city  + 'cov_mat_axis.csv')\n",
    "    cov_mat = pd.DataFrame(index = cov_mat_ind['rows'].values, \n",
    "                           columns=cov_mat_ind['columns'].values, \n",
    "                           data = cov_mat)\n",
    "    area_seg = compute_size_seg(zones, city, cov_mat)\n",
    "    area_seg_tot = pd.concat([area_seg_tot,area_seg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_seg_tot = gpd.GeoDataFrame(data = area_seg_tot,geometry = area_seg_tot['geometry'],crs = 'EPSG:28992')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_seg_tot = area_seg_tot.rename(columns = {'mean_ind':'expos_NW_reg'})\n",
    "area_seg_tot = area_seg_tot.drop(columns = ['weighted_expos','expos_rel','access_NW','access_tot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lulu/.pyenv/versions/3.9.7/envs/study_1_venv/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "area_seg_tot.to_file('../data/results/regions/regions.gpkg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('study_1_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11eb3bdda620b386ac311b3d07c097973ef5c99cb9184bf1a093190cd8996fdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
