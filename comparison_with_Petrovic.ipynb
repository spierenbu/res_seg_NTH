{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from scipy.stats import norm \n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import string\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_char=list(string.ascii_uppercase)\n",
    "alphabet=list_char+[x+y for x in list_char for y in list_char]\n",
    "name_columns = alphabet[0:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_cell = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example = pd.read_csv('../data/toy_example/toy_example_indicators.csv', names= name_columns)\n",
    "toy_example.index = name_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example_1 = toy_example * 0.3\n",
    "toy_example_2 = toy_example * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of a grid.\n",
    "def create_grid_geometry(city_mix, size_cell, name_columns):\n",
    "\n",
    "    N_rows = len(city_mix)\n",
    "    polygons = []\n",
    "    polygon_id = []\n",
    "\n",
    "    for x in range(N_rows):\n",
    "        for y in range(N_rows):\n",
    "            polygons.append(Polygon([(size_cell*x,size_cell*y), \n",
    "                                    (size_cell*(x+1), size_cell*y), \n",
    "                                    (size_cell*(x+1), size_cell*(y+1)),\n",
    "                                    (size_cell*x, size_cell*(y+1))]))\n",
    "            polygon_id.append(name_columns[x] + '_' + name_columns[y])\n",
    "\n",
    "    grid = gpd.GeoDataFrame({'polygon_id':polygon_id,'geometry':polygons})\n",
    "    \n",
    "    centroids = grid.copy()\n",
    "    centroids['geometry'] = grid.centroid\n",
    "\n",
    "    return grid, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid, centroids = create_grid_geometry(toy_example, size_cell, name_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demographics(demographics, grid):\n",
    "\n",
    "    demographics_stacked = demographics.T.stack().reset_index()\n",
    "    demographics_stacked = demographics_stacked.rename(columns = {0:'res_mix'})\n",
    "    demographics_stacked['polygon_id'] = demographics_stacked['level_0'] + '_' + demographics_stacked['level_1']\n",
    "    demographics_stacked = demographics_stacked.drop(columns = ['level_0','level_1'])\n",
    "\n",
    "    return grid.merge(demographics_stacked, on = 'polygon_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example_1 = add_demographics(toy_example_1, grid)\n",
    "toy_example_2 = add_demographics(toy_example_2, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(grid, distance):\n",
    "\n",
    "    grid_mask = grid.copy()\n",
    "    grid_mask['geometry'] = grid_mask['geometry'].centroid.buffer(distance)\n",
    "    grid_mask = grid_mask.drop(columns = 'res_mix').rename(columns ={'polygon_id':'from_polygon_id'})\n",
    "\n",
    "    grid_target = grid.copy()\n",
    "    grid_target['geometry'] = grid_target['geometry'].centroid\n",
    "    grid_target = grid_target.rename(columns ={'polygon_id':'to_polygon_id'})\n",
    "\n",
    "    grid_mask = grid_mask.sjoin(grid_target).drop(columns = 'index_right')\n",
    "    grid_mask = grid_mask.reset_index(drop = True)\n",
    "\n",
    "    return grid_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shortest_paths(res_mix,size_cell, grid):\n",
    "    # Computing the shortest paths from cell to cell.\n",
    "    grid = grid.copy()\n",
    "    grid['geometry'] = grid['geometry'].centroid\n",
    "    shortest_paths = pd.DataFrame({'from_polygon_id':[],'to_polygon_id':[],'distance':[]})\n",
    "\n",
    "    for i in res_mix['polygon_id']:\n",
    "        shortest_paths_i = pd.DataFrame({'from_polygon_id':[i]*len(res_mix),\n",
    "                                         'to_polygon_id':res_mix['polygon_id'],\n",
    "                                         'distance':grid.distance(grid.loc[res_mix['polygon_id'] == i,'geometry'].values[0])})\n",
    "        shortest_paths = pd.concat([shortest_paths,shortest_paths_i], ignore_index=True)\n",
    "    \n",
    "    shortest_paths['weight'] = 1/(0.3*size_cell*0.707)**2\n",
    "    shortest_paths['weight'] = shortest_paths['weight'].mask(shortest_paths['distance'] > 0,\n",
    "                                                             1/(0.3*shortest_paths['distance'])**2)\n",
    "\n",
    "    return shortest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_paths = compute_shortest_paths(toy_example_1,size_cell, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exposure(toy_example,r,size_cell):\n",
    "    \n",
    "    toy_example = toy_example.copy()\n",
    "    grid_mask = create_mask(toy_example, size_cell*r)\n",
    "\n",
    "    weight_Petrovic = grid_mask[['from_polygon_id','to_polygon_id']].groupby('from_polygon_id').count().reset_index()\n",
    "    weight_Petrovic = weight_Petrovic.rename(columns = {'to_polygon_id':'weight_Petrovic'})\n",
    "    exposure_Petrovic = grid_mask.groupby('from_polygon_id').mean().reset_index()\n",
    "    exposure_Petrovic = exposure_Petrovic.merge(weight_Petrovic, on ='from_polygon_id')\n",
    "    exposure_Petrovic = exposure_Petrovic.rename(columns = {'from_polygon_id':'polygon_id','res_mix':'exposure_Petrovic'})\n",
    "\n",
    "    grid_mask_Lan = grid_mask.merge(shortest_paths.drop(columns = 'weight'), on = ['from_polygon_id','to_polygon_id'])\n",
    "    grid_mask_Lan['weight'] = 1\n",
    "    grid_mask_Lan['weight'] = grid_mask_Lan['weight'].mask(grid_mask_Lan['distance'] > 0, \n",
    "                                                        (1- (grid_mask_Lan['distance']/size_cell/r)**2) ** 2)\n",
    "    grid_mask_Lan['exposure'] = grid_mask_Lan['weight'] * grid_mask['res_mix'] \n",
    "    exposure_Lan = grid_mask_Lan.groupby('from_polygon_id').sum().reset_index()\n",
    "    exposure_Lan['exposure'] = exposure_Lan['exposure'] / exposure_Lan['weight']\n",
    "    exposure_Lan = exposure_Lan.rename(columns = {'from_polygon_id':'polygon_id','exposure':'exposure_Lan','weight':'weight_Lan'})\n",
    "    toy_example = toy_example.merge(exposure_Lan[['polygon_id','exposure_Lan','weight_Lan']], on = 'polygon_id')\n",
    "    toy_example = toy_example.merge(exposure_Petrovic[['polygon_id','exposure_Petrovic','weight_Petrovic']], on = 'polygon_id')\n",
    "\n",
    "\n",
    "    return toy_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(table, column):\n",
    "    table['entropy_' + column] = 0\n",
    "    table['entropy_' + column] = table['entropy_' + column].mask((table[column] > 0) & (table[column] < 1),\n",
    "                                                                  table[column] * np.log2(table[column]) + \n",
    "                                                                  (1-table[column]) * np.log2(1 - table[column]))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 3\n",
    "exposure = compute_exposure(toy_example_1,r,size_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mix_city = exposure['res_mix'].sum()/len(exposure)\n",
    "entropy_city = -(res_mix_city * np.log2(res_mix_city) + (1 - res_mix_city) * np.log2((1 - res_mix_city)))\n",
    "\n",
    "exposure = compute_entropy(exposure, 'res_mix')\n",
    "entropy_res_mix = 1 + exposure['entropy_res_mix'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "exposure = compute_entropy(exposure, 'exposure_Petrovic')\n",
    "entropy_Petrovic = 1 + exposure['entropy_exposure_Petrovic'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "exposure = compute_entropy(exposure, 'exposure_Lan')\n",
    "entropy_Lan = 1 + exposure['entropy_exposure_Lan'].sum()/len(exposure)/entropy_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_1 = pd.DataFrame({'distance':[],'entropy_Petrovic':[],'entropy_Lan':[]})\n",
    "evolution_2 = pd.DataFrame({'distance':[],'entropy_Petrovic':[],'entropy_Lan':[]})\n",
    "\n",
    "for r in range(1,8):\n",
    "\n",
    "    exposure = compute_exposure(toy_example_1,r,size_cell)\n",
    "    res_mix_city = exposure['res_mix'].sum()/len(exposure)\n",
    "    entropy_city = -(res_mix_city * np.log2(res_mix_city) + (1 - res_mix_city) * np.log2((1 - res_mix_city)))\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'res_mix')\n",
    "    entropy_res_mix = 1 + exposure['entropy_res_mix'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'exposure_Petrovic')\n",
    "    entropy_Petrovic = 1 + exposure['entropy_exposure_Petrovic'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'exposure_Lan')\n",
    "    entropy_Lan = 1 + exposure['entropy_exposure_Lan'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    evolution_1 = pd.concat([evolution_1,\n",
    "                             pd.DataFrame({'distance':[r],\n",
    "                                           'entropy_Petrovic':[entropy_Petrovic],\n",
    "                                           'entropy_Lan':[entropy_Lan]})], ignore_index = True)\n",
    "\n",
    "\n",
    "for r in range(1,8):\n",
    "\n",
    "    exposure = compute_exposure(toy_example_2,r,size_cell)\n",
    "    res_mix_city = exposure['res_mix'].sum()/len(exposure)\n",
    "    entropy_city = -(res_mix_city * np.log2(res_mix_city) + (1 - res_mix_city) * np.log2((1 - res_mix_city)))\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'res_mix')\n",
    "    entropy_res_mix = 1 + exposure['entropy_res_mix'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'exposure_Petrovic')\n",
    "    entropy_Petrovic = 1 + exposure['entropy_exposure_Petrovic'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    exposure = compute_entropy(exposure, 'exposure_Lan')\n",
    "    entropy_Lan = 1 + exposure['entropy_exposure_Lan'].sum()/len(exposure)/entropy_city\n",
    "\n",
    "    evolution_2 = pd.concat([evolution_2,\n",
    "                             pd.DataFrame({'distance':[r],\n",
    "                                           'entropy_Petrovic':[entropy_Petrovic],\n",
    "                                           'entropy_Lan':[entropy_Lan]})], ignore_index = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(8, 4), layout = 'constrained')\n",
    "\n",
    "# Fine grid\n",
    "ax[0].plot(evolution_1['distance'], evolution_1['entropy_Petrovic'], label = 'City 1')\n",
    "ax[0].plot(evolution_2['distance'],evolution_2['entropy_Petrovic'], label = 'City 2')\n",
    "ax[0].set_ylim(0,evolution_2['entropy_Lan'].max())\n",
    "ax[0].set_xlabel('Radius of the local environment')\n",
    "ax[0].set_ylabel('Information index using the method of Petrovic')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(evolution_1['distance'],evolution_1['entropy_Lan'], label = 'City 1')\n",
    "ax[1].plot(evolution_2['distance'],evolution_2['entropy_Lan'], label = 'City 2')\n",
    "ax[1].set_xlabel('Radius of the local environment')\n",
    "ax[1].set_ylabel('Information index using the method of Lan')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(8, 3.2), layout = 'constrained')\n",
    "\n",
    "toy_example_1.plot(ax=ax[0], cmap = 'Blues', column = 'res_mix', vmin = 0, vmax = 1)\n",
    "toy_example_1.plot(ax=ax[0],color = 'none', edgecolor = 'black')\n",
    "toy_example_2.plot(ax=ax[1], cmap = 'Blues', column = 'res_mix', vmin = 0, vmax = 1)\n",
    "toy_example_2.plot(ax=ax[1],color = 'none', edgecolor = 'black')\n",
    "\n",
    "# Set titles\n",
    "ax[0].set_title('City 1', size = 15)\n",
    "ax[1].set_title('City 2', size = 15)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "\n",
    "fig.colorbar(cm.ScalarMappable(norm = mpl.colors.Normalize(vmin=-0, vmax=1), cmap='Blues'),\n",
    "                               ax=ax[1], \n",
    "                               shrink=1).set_label(label='Group share',size=12)\n",
    "#grid_hh_data.plot(ax=ax[1],color = 'none', edgecolor = 'black')\n",
    "\n",
    "# ax[0].plot(evolution_1['distance'], evolution_1['entropy_Petrovic'], label = 'City 1')\n",
    "# ax[0].plot(evolution_2['distance'],evolution_2['entropy_Petrovic'], label = 'City 2')\n",
    "# ax[0].set_ylim(0,evolution_2['entropy_Lan'].max())\n",
    "# ax[0].set_xlabel('Radius of the local environment')\n",
    "# ax[0].set_ylabel('Information index using the method of Petrovic')\n",
    "# ax[0].legend()\n",
    "\n",
    "# ax[1].plot(evolution_1['distance'],evolution_1['entropy_Lan'], label = 'City 1')\n",
    "# ax[1].plot(evolution_2['distance'],evolution_2['entropy_Lan'], label = 'City 2')\n",
    "# ax[1].set_xlabel('Radius of the local environment')\n",
    "# ax[1].set_ylabel('Information index using the method of Lan')\n",
    "# ax[1].legend()\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11eb3bdda620b386ac311b3d07c097973ef5c99cb9184bf1a093190cd8996fdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
